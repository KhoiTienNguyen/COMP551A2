{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hEyjVRY2M5EL"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "training20=datasets.fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnJPcqNFNG6v",
        "outputId": "db69cda9-1742-4068-9473-ac581dba2d17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 101631)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#convert text to feature vectors\n",
        "import sklearn\n",
        "\n",
        "count_vect = sklearn.feature_extraction.text.CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(training20.data)\n",
        "X_train_counts.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-zjIcTeNdcy"
      },
      "source": [
        "TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nb--WHkKNWrz"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse.lil import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kRv4GWhANfzs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "twenty_test = datasets.fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "X_new_counts = count_vect.transform(twenty_test.data)\n",
        "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o1wPg7JHOZMt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(1234)\n",
        "\n",
        "\n",
        "class MultinomialNaiveBayes:\n",
        "    \n",
        "    def __init__(self):\n",
        "        return\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "      train = x\n",
        "      doc, features = train.shape\n",
        "      # Prior\n",
        "      unique, counts = np.unique(y, return_counts=True)\n",
        "      prior = dict(zip(unique, counts))\n",
        "      vocab = {}\n",
        "      hm = {}\n",
        "      for i in prior:\n",
        "        prior[i] /= doc\n",
        "        hm[i] = {}\n",
        "        vocab[i] = 0\n",
        "      # Conditional probabilities\n",
        "      # Create a dictionary with every class in it\n",
        "      for j in range(len(train)):\n",
        "        vocab[y[j]] += np.count_nonzero(train[j])\n",
        "        if j % 500 == 0:\n",
        "          print(j)\n",
        "        for k in range(features):\n",
        "          #smoothing\n",
        "          if k not in hm[y[j]]:\n",
        "            hm[y[j]][k] = 1\n",
        "          hm[y[j]][k] += (train[j][k])\n",
        "      for i in hm:\n",
        "        for j in hm[i]:\n",
        "          hm[i][j] /= (doc+vocab[i])\n",
        "      self.hm = hm\n",
        "      self.prior = prior\n",
        "\n",
        "      return self\n",
        "\n",
        "def logsumexp(Z):                                                # dimension C x N\n",
        "    Zmax = np.max(Z,axis=0)[None,:]                              # max over C\n",
        "    log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=0))\n",
        "    return log_sum_exp\n",
        "\n",
        "def predict(self, xt):\n",
        "    from math import log, exp\n",
        "    result = []\n",
        "    first = list(self.hm.keys())[0]\n",
        "    # for each line to predict\n",
        "    for i in range(len(xt)):\n",
        "      # for each class it could be\n",
        "      tmp = first\n",
        "      best = float('-inf')\n",
        "      for j in self.hm:\n",
        "        # calculate probability of that line being that class\n",
        "        # and then choose the class with highest probability\n",
        "        prob = log(self.prior[j])\n",
        "        for k in range(len(xt[i])):\n",
        "          if xt[i][k] != 0.0:\n",
        "            prob += log(xt[i][k]) + log(self.hm[j][k])\n",
        "        if best < prob:\n",
        "          best = prob\n",
        "          tmp = j\n",
        "      result.append(tmp)\n",
        "    res = np.array(result)\n",
        "    return res                                      # dimension N x C\n",
        "\n",
        "MultinomialNaiveBayes.predict = predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjQlSVeezgUt"
      },
      "source": [
        "Softmax regression for 20newsgroup dataset with different parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZqguX3TqGf2",
        "outputId": "a8722b31-d55e-4d6c-d75e-c39dd6d3fbab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:705: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.722821 using {'C': 100, 'solver': 'liblinear'}\n",
            "0.716369 (0.002033) with: {'C': 100, 'solver': 'newton-cg'}\n",
            "0.715927 (0.002298) with: {'C': 100, 'solver': 'lbfgs'}\n",
            "0.722821 (0.004243) with: {'C': 100, 'solver': 'liblinear'}\n",
            "0.714867 (0.000354) with: {'C': 10, 'solver': 'newton-cg'}\n",
            "0.714955 (0.000088) with: {'C': 10, 'solver': 'lbfgs'}\n",
            "0.720170 (0.000177) with: {'C': 10, 'solver': 'liblinear'}\n",
            "0.692505 (0.000972) with: {'C': 1.0, 'solver': 'newton-cg'}\n",
            "0.692593 (0.001061) with: {'C': 1.0, 'solver': 'lbfgs'}\n",
            "0.691621 (0.001679) with: {'C': 1.0, 'solver': 'liblinear'}\n",
            "0.565229 (0.003977) with: {'C': 0.1, 'solver': 'newton-cg'}\n",
            "0.565229 (0.003977) with: {'C': 0.1, 'solver': 'lbfgs'}\n",
            "0.565141 (0.004950) with: {'C': 0.1, 'solver': 'liblinear'}\n",
            "0.429291 (0.003977) with: {'C': 0.01, 'solver': 'newton-cg'}\n",
            "0.429203 (0.004066) with: {'C': 0.01, 'solver': 'lbfgs'}\n",
            "0.448824 (0.003005) with: {'C': 0.01, 'solver': 'liblinear'}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "softmax=LogisticRegression()\n",
        "#softmax.fit(X_train_tfidf, training20.target)\n",
        "#predicted=softmax.predict(X_new_tfidf)\n",
        "#np.mean(predicted==twenty_test.target)\n",
        "\n",
        "\n",
        "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
        "#penalty = ['l2']\n",
        "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
        "# define grid search\n",
        "grid = dict(solver=solvers,C=c_values)\n",
        "cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=1, random_state=1)\n",
        "grid_search = GridSearchCV(estimator=softmax, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
        "grid_result = grid_search.fit(X_train_tfidf, training20.target)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG7yVaDYi8EW"
      },
      "source": [
        "sentiment dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXMY85IsfrJS",
        "outputId": "fe42a58b-9774-4ac1-a26a-5baf70c1d9d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "#import sentiment140 dataset into pandas\n",
        "import pandas as pd\n",
        "import csv\n",
        "col=[\"sentiment\", \"tweetID\", \"date\", \"query\", \"user\", \"text\"]\n",
        "df=pd.read_csv(\"/content/training.1600000.processed.noemoticon.csv\", \n",
        "               encoding='latin-1', engine='python', error_bad_lines=False, names=col)\n",
        "from sklearn.utils import shuffle\n",
        "df = shuffle(df,random_state=0)\n",
        "sentiment_df = df[\"sentiment\"]\n",
        "text_df = df[\"text\"]\n",
        "sentiment = sentiment_df[:10000].to_numpy()\n",
        "text = text_df[:10000].to_numpy()\n",
        "\n",
        "#convert text to feature vectors\n",
        "import sklearn\n",
        "# from scipy.sparse.lil import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# count_vect2 = sklearn.feature_extraction.text.CountVectorizer(stop_words='english',ngram_range=(1,1))\n",
        "count_vect2 = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1))\n",
        "X_train_counts2 = count_vect2.fit_transform(text)\n",
        "# X_train_counts.shape\n",
        "\n",
        "tfidf_transformer2 = TfidfTransformer()\n",
        "X_train_tfidf2 = tfidf_transformer2.fit_transform(X_train_counts2)\n",
        "# X_train_tfidf.shape\n",
        "X_train_tfidf2\n",
        "\n",
        "#import sentiment140 TEST set into pandas\n",
        "import pandas as pd\n",
        "import csv\n",
        "col=[\"sentiment\", \"tweetID\", \"date\", \"query\", \"user\", \"text\"]\n",
        "df2=pd.read_csv(\"/content/new_testdata.manual.2009.06.14.csv\", \n",
        "               encoding='latin-1', engine='python', error_bad_lines=False, names=col)\n",
        "\n",
        "\n",
        "test_text = df2[\"text\"]\n",
        "test_sentiment = df2[\"sentiment\"]\n",
        "\n",
        "t_text = test_text.to_numpy()\n",
        "t_sentiment = test_sentiment.to_numpy()\n",
        "\n",
        "import numpy as np\n",
        "X_new_counts2 = count_vect2.transform(t_text)\n",
        "X_new_tfidf2 = tfidf_transformer2.transform(X_new_counts2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOcrRF4Zo3UP"
      },
      "source": [
        "softmax regression for sentiment dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSnOHWcKi_uT",
        "outputId": "49949563-c73d-4af3-9eeb-bf284ac9b814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.740000 using {'C': 1.0, 'solver': 'newton-cg'}\n",
            "0.720200 (0.002800) with: {'C': 100, 'solver': 'newton-cg'}\n",
            "0.720100 (0.002700) with: {'C': 100, 'solver': 'lbfgs'}\n",
            "0.720200 (0.002800) with: {'C': 100, 'solver': 'liblinear'}\n",
            "0.733900 (0.000300) with: {'C': 10, 'solver': 'newton-cg'}\n",
            "0.733800 (0.000400) with: {'C': 10, 'solver': 'lbfgs'}\n",
            "0.733800 (0.000400) with: {'C': 10, 'solver': 'liblinear'}\n",
            "0.740000 (0.001600) with: {'C': 1.0, 'solver': 'newton-cg'}\n",
            "0.739900 (0.001500) with: {'C': 1.0, 'solver': 'lbfgs'}\n",
            "0.739800 (0.001200) with: {'C': 1.0, 'solver': 'liblinear'}\n",
            "0.714000 (0.004000) with: {'C': 0.1, 'solver': 'newton-cg'}\n",
            "0.714000 (0.004000) with: {'C': 0.1, 'solver': 'lbfgs'}\n",
            "0.714400 (0.004400) with: {'C': 0.1, 'solver': 'liblinear'}\n",
            "0.665700 (0.004300) with: {'C': 0.01, 'solver': 'newton-cg'}\n",
            "0.665700 (0.004300) with: {'C': 0.01, 'solver': 'lbfgs'}\n",
            "0.675600 (0.004000) with: {'C': 0.01, 'solver': 'liblinear'}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "softmax2=LogisticRegression()\n",
        "softmax2.fit(X_train_tfidf2, sentiment)\n",
        "p=softmax2.predict(X_new_tfidf2)\n",
        "np.mean(p == t_sentiment)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#softmax=LogisticRegression()\n",
        "#softmax.fit(X_train_tfidf, training20.target)\n",
        "#predicted=softmax.predict(X_new_tfidf)\n",
        "#np.mean(predicted==twenty_test.target)\n",
        "\n",
        "\n",
        "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
        "#penalty = ['l2']\n",
        "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
        "# define grid search\n",
        "grid = dict(solver=solvers,C=c_values)\n",
        "cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=1, random_state=0)\n",
        "grid_search = GridSearchCV(estimator=softmax2, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
        "grid_result = grid_search.fit(X_train_tfidf2, sentiment)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Softmax.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}