{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "551MP2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "IMPORT FIRST DATASET"
      ],
      "metadata": {
        "id": "kGGl-PZZu5hc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSDeOOXk4g7O"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "import random as r\n",
        "r.seed(30)\n",
        "training20=datasets.fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "# training20\n",
        "#convert train text to feature vectors\n",
        "import sklearn\n",
        "# NGRAM CHANGE IT\n",
        "count_vect = sklearn.feature_extraction.text.CountVectorizer(stop_words='english',ngram_range=(1,1))\n",
        "# count_vect = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1))\n",
        "X_train_counts = count_vect.fit_transform(training20.data)\n",
        "#Convert train count vectors to tfidf\n",
        "from scipy.sparse.lil import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "import numpy as np\n",
        "#Convert test set to vectors to tfidf\n",
        "twenty_test = datasets.fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "X_new_counts = count_vect.transform(twenty_test.data)\n",
        "X_new_tfidf = tfidf_transformer.transform(X_new_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORT SECOND DATASET"
      ],
      "metadata": {
        "id": "U2sEufIgu7gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import sentiment140 dataset into pandas\n",
        "import pandas as pd\n",
        "import csv\n",
        "col=[\"sentiment\", \"tweetID\", \"date\", \"query\", \"user\", \"text\"]\n",
        "df=pd.read_csv(\"/content/training.1600000.processed.noemoticon.csv\", \n",
        "               encoding='latin-1', engine='python', error_bad_lines=False, names=col)\n",
        "from sklearn.utils import shuffle\n",
        "df = shuffle(df,random_state=0)\n",
        "sentiment_df = df[\"sentiment\"]\n",
        "text_df = df[\"text\"]\n",
        "sentiment = sentiment_df[:10000].to_numpy()\n",
        "text = text_df[:10000].to_numpy()\n",
        "\n",
        "#convert text to feature vectors\n",
        "import sklearn\n",
        "# from scipy.sparse.lil import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# count_vect2 = sklearn.feature_extraction.text.CountVectorizer(stop_words='english',ngram_range=(1,1))\n",
        "count_vect2 = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1))\n",
        "X_train_counts2 = count_vect2.fit_transform(text)\n",
        "# X_train_counts.shape\n",
        "\n",
        "tfidf_transformer2 = TfidfTransformer()\n",
        "X_train_tfidf2 = tfidf_transformer2.fit_transform(X_train_counts2)\n",
        "# X_train_tfidf.shape\n",
        "X_train_tfidf2\n",
        "\n",
        "#import sentiment140 TEST set into pandas\n",
        "import pandas as pd\n",
        "import csv\n",
        "col=[\"sentiment\", \"tweetID\", \"date\", \"query\", \"user\", \"text\"]\n",
        "df2=pd.read_csv(\"/content/new_testdata.manual.2009.06.14.csv\", \n",
        "               encoding='latin-1', engine='python', error_bad_lines=False, names=col)\n",
        "test_text = df2[\"text\"]\n",
        "test_sentiment = df2[\"sentiment\"]\n",
        "\n",
        "t_text = test_text.to_numpy()\n",
        "t_sentiment = test_sentiment.to_numpy()\n",
        "\n",
        "import numpy as np\n",
        "X_new_counts2 = count_vect2.transform(t_text)\n",
        "X_new_tfidf2 = tfidf_transformer2.transform(X_new_counts2)\n",
        "# docs_test = twenty_test.data\n",
        "# docs_test\n",
        "X_new_tfidf2\n",
        "# twenty_test.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O43JwoPkOEKK",
        "outputId": "70c003da-eccb-41e7-e9fb-90a162caf2de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<359x18731 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 4105 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "hxkN39aWK-Lv",
        "outputId": "6ecdcaf6-35b8-445a-949f-f0e45ace00a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-36454b05-de1f-4c2d-bd6f-5ee5a70e224a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>tweetID</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>557138</th>\n",
              "      <td>0</td>\n",
              "      <td>2204444171</td>\n",
              "      <td>Wed Jun 17 02:14:00 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>einmensch</td>\n",
              "      <td>wants to compete! i want hard competition! i w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349381</th>\n",
              "      <td>0</td>\n",
              "      <td>2017152437</td>\n",
              "      <td>Wed Jun 03 07:56:34 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>keithmorrison</td>\n",
              "      <td>It seems we are stuck on the ground in Amarill...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182051</th>\n",
              "      <td>0</td>\n",
              "      <td>1967043408</td>\n",
              "      <td>Fri May 29 18:52:13 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>PunkieDory</td>\n",
              "      <td>where the f are my pinking shears? rarararrrar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571236</th>\n",
              "      <td>0</td>\n",
              "      <td>2208721054</td>\n",
              "      <td>Wed Jun 17 09:32:48 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>DYkEY_tYPE</td>\n",
              "      <td>0ff t0 tHE MEEtiN..  i HAtE WhEN PPl V0lUNtEER...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1339637</th>\n",
              "      <td>4</td>\n",
              "      <td>2018731586</td>\n",
              "      <td>Wed Jun 03 10:25:27 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>BlueSmartiies</td>\n",
              "      <td>@ reply me pls</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152315</th>\n",
              "      <td>0</td>\n",
              "      <td>1932709242</td>\n",
              "      <td>Tue May 26 22:02:34 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Greenskull</td>\n",
              "      <td>@majornelson LOVING the zune, hating the lack ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>963395</th>\n",
              "      <td>4</td>\n",
              "      <td>1827437924</td>\n",
              "      <td>Sun May 17 10:42:36 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>adbruin</td>\n",
              "      <td>@kahilee Sounds like it was a blast! Have a sa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117952</th>\n",
              "      <td>0</td>\n",
              "      <td>1827736749</td>\n",
              "      <td>Sun May 17 11:20:21 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>la_discoteca</td>\n",
              "      <td>@marchingstars i feel bad for the zines i stil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1484405</th>\n",
              "      <td>4</td>\n",
              "      <td>2067663968</td>\n",
              "      <td>Sun Jun 07 12:44:35 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>J_Kaye</td>\n",
              "      <td>@ketadiablo - Just finished DUST AND MOONLIGHT...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305711</th>\n",
              "      <td>0</td>\n",
              "      <td>2000060304</td>\n",
              "      <td>Mon Jun 01 21:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>gisuck</td>\n",
              "      <td>@rlangdon .... but I want it to be real. You w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600000 rows Ã— 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36454b05-de1f-4c2d-bd6f-5ee5a70e224a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-36454b05-de1f-4c2d-bd6f-5ee5a70e224a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-36454b05-de1f-4c2d-bd6f-5ee5a70e224a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         sentiment     tweetID                          date     query  \\\n",
              "557138           0  2204444171  Wed Jun 17 02:14:00 PDT 2009  NO_QUERY   \n",
              "349381           0  2017152437  Wed Jun 03 07:56:34 PDT 2009  NO_QUERY   \n",
              "182051           0  1967043408  Fri May 29 18:52:13 PDT 2009  NO_QUERY   \n",
              "571236           0  2208721054  Wed Jun 17 09:32:48 PDT 2009  NO_QUERY   \n",
              "1339637          4  2018731586  Wed Jun 03 10:25:27 PDT 2009  NO_QUERY   \n",
              "...            ...         ...                           ...       ...   \n",
              "152315           0  1932709242  Tue May 26 22:02:34 PDT 2009  NO_QUERY   \n",
              "963395           4  1827437924  Sun May 17 10:42:36 PDT 2009  NO_QUERY   \n",
              "117952           0  1827736749  Sun May 17 11:20:21 PDT 2009  NO_QUERY   \n",
              "1484405          4  2067663968  Sun Jun 07 12:44:35 PDT 2009  NO_QUERY   \n",
              "305711           0  2000060304  Mon Jun 01 21:19:57 PDT 2009  NO_QUERY   \n",
              "\n",
              "                  user                                               text  \n",
              "557138       einmensch  wants to compete! i want hard competition! i w...  \n",
              "349381   keithmorrison  It seems we are stuck on the ground in Amarill...  \n",
              "182051      PunkieDory  where the f are my pinking shears? rarararrrar...  \n",
              "571236      DYkEY_tYPE  0ff t0 tHE MEEtiN..  i HAtE WhEN PPl V0lUNtEER...  \n",
              "1339637  BlueSmartiies                                    @ reply me pls   \n",
              "...                ...                                                ...  \n",
              "152315      Greenskull  @majornelson LOVING the zune, hating the lack ...  \n",
              "963395         adbruin  @kahilee Sounds like it was a blast! Have a sa...  \n",
              "117952    la_discoteca  @marchingstars i feel bad for the zines i stil...  \n",
              "1484405         J_Kaye  @ketadiablo - Just finished DUST AND MOONLIGHT...  \n",
              "305711          gisuck  @rlangdon .... but I want it to be real. You w...  \n",
              "\n",
              "[1600000 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Naive Bayes implementation"
      ],
      "metadata": {
        "id": "nMkL6xe7cWWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "# np.random.seed(1234)\n",
        "\n",
        "\n",
        "class GaussianNaiveBayes:\n",
        "    \n",
        "    def __init__(self):\n",
        "        return\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        N, D = x.shape\n",
        "        #C = np.max(y) + 1\n",
        "        #C=2\n",
        "        unique=np.unique(y)\n",
        "        C=len(unique)\n",
        "\n",
        "        # one parameter for each feature conditioned on each class\n",
        "        mu, sigma = np.zeros((C,D)), np.zeros((C,D))\n",
        "        Nc = np.zeros(C) # number of instances in class c\n",
        "\n",
        "        # for each class get the MLE for the mean and std\n",
        "        c=0;\n",
        "        for u in unique:\n",
        "            x_c = x[y == u]                           #slice all the elements from class c\n",
        "\n",
        "            Nc[c] = x_c.shape[0]                      #get number of elements of class c\n",
        "            mu[c,:] = np.mean(x_c,0)                  #mean of features of class c\n",
        "            sigma[c,:] = 1e-9 + np.std(x_c, 0)               #std of features of class c\n",
        "            c+=1;\n",
        "            #sigma[c%3,:] = np.std(x_c, 0)\n",
        "\n",
        "            #Nc[c%3] = x_c.shape[0]                      #get number of elements of class c\n",
        "            #mu[c%3,:] = np.mean(x_c,0)                  #mean of features of class c\n",
        "            #sigma[c%3,:] = 1e-9 + np.std(x_c, 0)               #std of features of class c\n",
        "            #sigma[c%3,:] = np.std(x_c, 0)\n",
        "            \n",
        "        self.mu = mu                                  # C x D\n",
        "        self.sigma = sigma                            # C x D\n",
        "        self.pi = (Nc)/(N)                        #Laplace smoothing (using alpha_c=1 for all c) you can derive using Dirichlet's distribution\n",
        "        return self\n",
        "\n",
        "def logsumexp(Z):                                                # dimension C x N\n",
        "    Zmax = np.max(Z,axis=0)[None,:]                              # max over C\n",
        "    log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=0))\n",
        "    return log_sum_exp\n",
        "\n",
        "def predict(self, xt):\n",
        "    Nt, D = xt.shape\n",
        "    # for numerical stability we work in the log domain\n",
        "    # we add a dimension because this is added to the log-likelihood matrix \n",
        "    # that assigns a likelihood for each class (C) to each test point, and so it is C x N\n",
        "    log_prior = np.log(self.pi)[:, None]\n",
        "    # logarithm of the likelihood term for Gaussian \n",
        "    # the first two terms are the logarithm of the normalization term in the Gaussian and the final term is the exponent in the Gaussian. \n",
        "    # Notice that we are adding dimensions (using None) to model parameters and data to make this evaluation. \n",
        "    # The reason is that sigma and mu are C x D, while the data x is N x D. We operate on a C x N x D shape by increasing the number of dimensions when needed\n",
        "    log_likelihood = -.5 * np.log(2*np.pi) - np.log(self.sigma[:,None,:]) -.5 * (((xt[None,:,:] - self.mu[:,None,:])/self.sigma[:,None,:])**2)\n",
        "    # now we sum over the feature dimension to get a C x N matrix (this has the log-likelihood for each class-test point combination)\n",
        "    log_likelihood = np.sum(log_likelihood, axis=2)\n",
        "    # posterior calculation\n",
        "    log_posterior = log_prior + log_likelihood\n",
        "    posterior = np.exp(log_posterior - logsumexp(log_posterior))\n",
        "    return np.argmax(posterior.T, 1)                                                  # dimension N x C\n",
        "\n",
        "GaussianNaiveBayes.predict = predict"
      ],
      "metadata": {
        "id": "ez3GFXCpcZqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_acc(model, X_train, y, X_test, y_test):\n",
        "  model = model\n",
        "  model.fit(X_train.toarray(), y)\n",
        "  p=model.predict(X_test[:].toarray())\n",
        "  accur=np.mean(p == y_test[:])\n",
        "  return accur\n",
        "\n"
      ],
      "metadata": {
        "id": "Oc76LzNbhTdw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import floor\n",
        "from scipy.sparse import vstack\n",
        "def cross_validation_split(train):\n",
        "  cv_list = [[],[],[],[],[]]\n",
        "  cv_split = floor(train.shape[0]/5)\n",
        "  for i in range(0,5):\n",
        "    x_train = vstack((train[:cv_split*i,], train[cv_split*(i+1):cv_split*5,]))\n",
        "    x_validation= train[cv_split*i:cv_split*(i+1),]\n",
        "    cv_list[i].append(x_train)\n",
        "    cv_list[i].append(x_validation)\n",
        "  return cv_list\n"
      ],
      "metadata": {
        "id": "S7tN4HEO3VC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import floor\n",
        "def cross_validation_split_target(train):\n",
        "  cv_list = [[],[],[],[],[]]\n",
        "  cv_split = floor(train.shape[0]/5)\n",
        "  for i in range(0,5):\n",
        "    x_train= np.concatenate((train[:cv_split*i,], train[cv_split*(i+1):cv_split*5,]))\n",
        "    x_validation= train[cv_split*i:cv_split*(i+1)]\n",
        "    cv_list[i].append(x_train)\n",
        "    cv_list[i].append(x_validation)\n",
        "  return cv_list\n"
      ],
      "metadata": {
        "id": "OVEfufzxEOGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_train_counts = cross_validation_split(X_train_counts)\n",
        "first_target = cross_validation_split_target(training20.target)\n",
        "first_train_tfidf = cross_validation_split(X_train_tfidf)"
      ],
      "metadata": {
        "id": "TvtnOVxVS6ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "second_train_counts = cross_validation_split(X_train_counts2)\n",
        "second_train_tfidf = cross_validation_split(X_train_tfidf2)\n",
        "second_target = cross_validation_split_target(sentiment)"
      ],
      "metadata": {
        "id": "bnPtKvJi7bm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Fold"
      ],
      "metadata": {
        "id": "e2u5NCLXSGHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def k_foldCV(model,train,target):\n",
        "  res = []\n",
        "  for i in range(len(train)):\n",
        "    m = model\n",
        "    # print(\"A\")\n",
        "    print\n",
        "    m.fit(train[i][0].toarray(),target[i][0])\n",
        "    # print(\"B\")\n",
        "    y_prob = m.predict(train[i][1].toarray())\n",
        "    # print(\"C\")\n",
        "    accuracy = np.mean(y_prob == target[i][1])\n",
        "    res.append(accuracy)\n",
        "    print(f'test accuracy: {accuracy}')\n",
        "  print(f'Total accuracy: {np.mean(res)*100}')"
      ],
      "metadata": {
        "id": "cw1_Ri79BYqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Fold but doesn't crash"
      ],
      "metadata": {
        "id": "W4-XYScaU4IQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import ceil\n",
        "def k_fold_memory(model,train,target):\n",
        "  res = []\n",
        "  for i in range(len(train)):\n",
        "    m = model\n",
        "    # print(\"A\")\n",
        "    print\n",
        "    m.fit(train[i][0].toarray(),target[i][0])\n",
        "    # print(\"B\")\n",
        "    count = 0\n",
        "    n=5\n",
        "    length = ceil(len(target[i][1])/n) + 1\n",
        "    # print(length)\n",
        "    for j in range(1,length):  \n",
        "      if j == length-1:\n",
        "        y_prob = model.predict(train[i][1][(j-1)*n:].toarray())\n",
        "        accuracy = np.sum(y_prob == target[i][1][(j-1)*n:])\n",
        "        count += accuracy\n",
        "      else:\n",
        "        y_prob = model.predict(train[i][1][(j-1)*n:j*n].toarray())\n",
        "        accuracy = np.sum(y_prob == target[i][1][(j-1)*n:j*n])\n",
        "        count += accuracy\n",
        "      # print(f'test accuracy: {accuracy}')\n",
        "    # print(\"C\")\n",
        "    acc = count / len(target[i][1])\n",
        "    res.append(acc)\n",
        "    print(f'test accuracy: {acc}')\n",
        "  print(f'Total accuracy: {np.mean(res)*100}')"
      ],
      "metadata": {
        "id": "M4ZzDovhU3ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = X_train_tfidf.toarray()\n",
        "y = training20.target\n",
        "doc, features = train.shape\n",
        "# Prior\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "prior = dict(zip(unique, counts))\n",
        "vocab = {}\n",
        "hm = {}\n",
        "for i in prior:\n",
        "  prior[i] /= doc\n",
        "  hm[i] = {}\n",
        "  vocab[i] = 0\n",
        "  for j in range(features):\n",
        "    hm[i][j] = 1\n",
        "# Conditional probabilities\n",
        "# Create a dictionary with every class in it\n",
        "for j in range(len(train)):\n",
        "  vocab[y[j]] += np.count_nonzero(train[j])\n",
        "  # if j % 500 == 0:\n",
        "  #   print(j)\n",
        "  nz = np.nonzero(train[j])[0]\n",
        "  for k in nz:\n",
        "    hm[y[j]][k] += (train[j][k])\n",
        "# for i in hm:\n",
        "#   for j in hm[i]:\n",
        "#     hm[i][j] /= (doc+vocab[i])"
      ],
      "metadata": {
        "id": "3xaq5DBjQ9LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in a:\n",
        "  print(count_vect2.get_feature_names_out()[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-Q9-bhAS42w",
        "outputId": "14518508-62ab-439e-899d-c762560f739c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good\n",
            "just\n",
            "love\n",
            "thanks\n",
            "day\n",
            "http\n",
            "lol\n",
            "going\n",
            "quot\n",
            "time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training20.target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1WnLrxvY_SN",
        "outputId": "632eb07f-2141-4437-9e53-a946fb7505db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  a = list(dict(sorted(hm[i].items(), key=lambda item: item[1], reverse=True)))[:1]\n",
        "  print(f'{training20.target_names[i]}: {count_vect.get_feature_names_out()[a[0]]} - {hm[i][a[0]]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB_C1Eg3Ru-J",
        "outputId": "5a8f8744-3886-43ff-cba5-221030bb1a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alt.atheism: god - 16.885143940439875\n",
            "comp.graphics: graphics - 22.27839115174076\n",
            "comp.os.ms-windows.misc: windows - 45.96148999171559\n",
            "comp.sys.ibm.pc.hardware: drive - 24.232572025030475\n",
            "comp.sys.mac.hardware: mac - 25.144832160160604\n",
            "comp.windows.x: window - 27.7852812266701\n",
            "misc.forsale: sale - 24.400559943390522\n",
            "rec.autos: car - 37.74108433156238\n",
            "rec.motorcycles: bike - 31.02666661344683\n",
            "rec.sport.baseball: year - 18.142359608916326\n",
            "rec.sport.hockey: game - 24.89440391749523\n",
            "sci.crypt: key - 36.21139172170438\n",
            "sci.electronics: use - 10.611051532699056\n",
            "sci.med: msg - 14.736067824761248\n",
            "sci.space: space - 33.71381166888871\n",
            "soc.religion.christian: god - 45.85983208903894\n",
            "talk.politics.guns: gun - 25.765047764124546\n",
            "talk.politics.mideast: israel - 30.542671837490897\n",
            "talk.politics.misc: people - 14.480029425502215\n",
            "talk.religion.misc: god - 15.066749386133079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-fold GaussianNB first dataset count vectorizer with stopwords"
      ],
      "metadata": {
        "id": "wqxjogFSSFnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_fold_memory(GaussianNaiveBayes(),first_train_counts,first_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo9V2N0wSE-K",
        "outputId": "e4ffda4a-6e31-4a87-d8d4-5db8b64689a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.6273209549071618\n",
            "test accuracy: 0.6317418213969939\n",
            "test accuracy: 0.6414677276746242\n",
            "test accuracy: 0.638815207780725\n",
            "test accuracy: 0.6304155614500442\n",
            "Total accuracy: 63.39522546419097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-fold GaussianNB first dataset tfidf with stopwords"
      ],
      "metadata": {
        "id": "ypo7-nm8g6Qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_fold_memory(GaussianNaiveBayes(),first_train_tfidf,first_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6xZYZC8g0WD",
        "outputId": "75812202-ea50-4622-f4bf-aac639a20b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.6282051282051282\n",
            "test accuracy: 0.6264367816091954\n",
            "test accuracy: 0.6423519009725907\n",
            "test accuracy: 0.6370468611847923\n",
            "test accuracy: 0.6361626878868258\n",
            "Total accuracy: 63.40406719717065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get rid of warnings"
      ],
      "metadata": {
        "id": "meJ8CWtb2qlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "fL7UsE0yjfC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-fold GaussianNB second dataset count vectorizor no stopwords"
      ],
      "metadata": {
        "id": "ifXIXmVEjvmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_fold(GaussianNaiveBayes(),second_train_counts,second_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yumQqw6iP9T",
        "outputId": "3eb0d44d-6e94-4b94-caf8-b6c76bd8fd5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.5005\n",
            "test accuracy: 0.5005\n",
            "test accuracy: 0.489\n",
            "test accuracy: 0.49\n",
            "test accuracy: 0.497\n",
            "Total accuracy: 49.53999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-fold GaussianNB second dataset tfidf no stopwords"
      ],
      "metadata": {
        "id": "QYrDiDAgj24R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_fold(GaussianNaiveBayes(),second_train_tfidf,second_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPe3ArE5j5El",
        "outputId": "1139403a-ba23-4cab-88f6-6b83dde3cf34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.5005\n",
            "test accuracy: 0.5005\n",
            "test accuracy: 0.489\n",
            "test accuracy: 0.49\n",
            "test accuracy: 0.497\n",
            "Total accuracy: 49.53999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-fold GaussianNB second dataset count vectorizor with stopwords"
      ],
      "metadata": {
        "id": "Txm0H6D3j3_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_fold(GaussianNaiveBayes(),second_train_counts,second_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cqQOaw4k-r5",
        "outputId": "f2df6600-2bf7-4788-a028-1253b22046b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.5005\n",
            "test accuracy: 0.5005\n",
            "test accuracy: 0.489\n",
            "test accuracy: 0.49\n",
            "test accuracy: 0.497\n",
            "Total accuracy: 49.53999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-fold GaussianNB second dataset count vectorizor with stopwords"
      ],
      "metadata": {
        "id": "pRk1sDMrj4e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_fold(GaussianNaiveBayes(),second_train_tfidf,second_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHyIt891lB8O",
        "outputId": "d802d9fc-3a66-4d62-d9a5-4b28e200acdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.5005\n",
            "test accuracy: 0.5005\n",
            "test accuracy: 0.489\n",
            "test accuracy: 0.49\n",
            "test accuracy: 0.497\n",
            "Total accuracy: 49.53999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-fold MultinomialNB second dataset count vectorizor with stopwords"
      ],
      "metadata": {
        "id": "Mlj4ebEel_Os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_fold(MultinomialNaiveBayes(),second_train_counts,second_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie1vtsQWmGlA",
        "outputId": "a82da9c6-ab8e-4eba-af50-9cace6fe8cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.7185\n",
            "test accuracy: 0.713\n",
            "test accuracy: 0.7115\n",
            "test accuracy: 0.6915\n",
            "test accuracy: 0.707\n",
            "Total accuracy: 70.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-fold MultinomialNB second dataset tfidf with stopwords"
      ],
      "metadata": {
        "id": "Zkqw2CWAmEHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_fold(MultinomialNaiveBayes(),second_train_tfidf,second_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M-egC50mRmm",
        "outputId": "bfa823c4-9681-499a-aa19-ff8e1c1a0614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.7255\n",
            "test accuracy: 0.7165\n",
            "test accuracy: 0.716\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.713\n",
            "Total accuracy: 71.42000000000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-fold MultinomialNB second dataset count vectorizor no stopwords"
      ],
      "metadata": {
        "id": "dLUK-tdDmEoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_fold(MultinomialNaiveBayes(),second_train_counts,second_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q63McVnmd4s",
        "outputId": "8bd93ad6-940d-4cb9-a697-e736750321cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.742\n",
            "test accuracy: 0.7235\n",
            "test accuracy: 0.7325\n",
            "test accuracy: 0.714\n",
            "test accuracy: 0.7345\n",
            "Total accuracy: 72.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-fold MultinomialNB second dataset tfidf no stopwords"
      ],
      "metadata": {
        "id": "1gAxbBFMmFEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_fold(MultinomialNaiveBayes(),second_train_tfidf,second_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdL1NtyymgL5",
        "outputId": "347d2af7-e4c3-436e-e511-f0df2bc2c1d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.7485\n",
            "test accuracy: 0.734\n",
            "test accuracy: 0.7385\n",
            "test accuracy: 0.725\n",
            "test accuracy: 0.7425\n",
            "Total accuracy: 73.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing (1,2) n-grams on small subset of second dataset"
      ],
      "metadata": {
        "id": "NjFBQkBqyg7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GaussianNaiveBayes()\n",
        "model.fit(X_train_tfidf2[:1000].toarray(), sentiment[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4Ffx12Cydsm",
        "outputId": "11ad177b-27e6-4284-95a4-64da991b6d02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in true_divide\n",
            "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:263: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  keepdims=keepdims, where=where)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in true_divide\n",
            "  subok=False)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:252: RuntimeWarning: invalid value encountered in true_divide\n",
            "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.GaussianNaiveBayes at 0x7fa38c0d8050>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(t_sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M31RwBAe09PN",
        "outputId": "4738f845-ef83-4057-bb1b-9c70e9f83df3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "359"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import ceil\n",
        "l = []\n",
        "n=10\n",
        "length = ceil(len(t_sentiment)/n) + 1\n",
        "count = 0\n",
        "for j in range(1,length):\n",
        "  if j == length-1:\n",
        "    y_prob = model.predict(X_new_tfidf2[(j-1)*n:].toarray())\n",
        "    accuracy = np.sum(y_prob == t_sentiment[(j-1)*n:])\n",
        "    count += accuracy\n",
        "  else:\n",
        "    y_prob = model.predict(X_new_tfidf2[(j-1)*n:j*n].toarray())\n",
        "    accuracy = np.sum(y_prob == t_sentiment[(j-1)*n:j*n])\n",
        "    count += accuracy\n",
        "  # print(f'test accuracy: {accuracy}')\n",
        "acc = count / 359\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enlsbrKYygN3",
        "outputId": "1bb35f3a-68af-4b05-bcd5-73fcfee65410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.49303621169916434"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result of (1,2) n-gram with MultinomialNB on second dataset with best hyperparameters: 62.40%"
      ],
      "metadata": {
        "id": "GZNEf14b3dXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result of (1,2) n-gram with GaussianNB on second dataset with best hyperparameters: 49.30%"
      ],
      "metadata": {
        "id": "7G_4qcBp34TL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result of (1,1) n-gram with MultinomialNB on second dataset with best hyperparameters: 68.80%"
      ],
      "metadata": {
        "id": "YD4FOl5g4eLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result of (1,1) n-gram with GaussianNB on second dataset with best hyperparameters: 49.30%"
      ],
      "metadata": {
        "id": "wTLyTj8b4veM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing (1,2) n-grams on small subset of first dataset"
      ],
      "metadata": {
        "id": "RHq2YjvE5pG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GaussianNaiveBayes()\n",
        "model.fit(X_train_tfidf[:].toarray(), training20.target[:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdNqKEYOpLRJ",
        "outputId": "a1d0d324-9880-4217-d12d-0363ed926fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.GaussianNaiveBayes at 0x7f88eee45450>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = []\n",
        "n=10\n",
        "for i in range(1,753):\n",
        "  y_prob = model.predict(X_new_tfidf[(i-1)*n:i*n].toarray())\n",
        "  accuracy = np.mean(y_prob == twenty_test.target[(i-1)*n:i*n])\n",
        "  l.append(accuracy)\n",
        "  print(f'test accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SW7NOGeTfmH",
        "outputId": "01f262dd-a361-43f6-9c54-c04e187e1ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.1\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.1\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.1\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 1.0\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.1\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result of (1,2) n-gram with MultinomialNB on first dataset with best hyperparameters: 29.74%"
      ],
      "metadata": {
        "id": "J6zDUtkfBGgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result of (1,2) n-gram with GaussianNB on first dataset with count and stopwords: 43.46%"
      ],
      "metadata": {
        "id": "4E5pz5ZIJz_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result of (1,2) n-gram with GaussianNB on first dataset with best hyperparameters: 44.60%"
      ],
      "metadata": {
        "id": "OuWydQXlFN2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#32\n",
        "np.mean(l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYONLIwdXBpH",
        "outputId": "038d4b96-9e3b-49ad-fd92-4dd5f8ff27e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5502659574468085"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GaussianNaiveBayes()\n",
        "y_prob = model.predict(X_new_tfidf2.toarray(), sentiment)\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "accuracy = np.sum(y_pred == t_sentiment)/y_pred.shape[0]\n",
        "l.append(accuracy)\n",
        "print(f'test accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "D-VM-fgTXfFo",
        "outputId": "eb97a798-367b-4fac-9e6c-ed3af6c3dd62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c365cbed307b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNaiveBayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new_tfidf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mt_sentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: predict() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing (1,1) n-gram on small subset (this is default) to compare to (1,2)"
      ],
      "metadata": {
        "id": "TthxiYavAmDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GaussianNaiveBayes()\n",
        "model.fit(X_train_counts[:].toarray(), training20.target[:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRz6BT6gAYAV",
        "outputId": "5e4dfa54-3b37-4244-94a6-7d372aee114c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.GaussianNaiveBayes at 0x7f06230de3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.sigma[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdvYtsYg_XTg",
        "outputId": "0ce9f111-7478-4412-a78f-4130fc1156e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.00000000e-09, 4.19319680e-01, 1.00000000e-09, ...,\n",
              "        1.00000000e-09, 1.00000000e-09, 1.00000000e-09],\n",
              "       [3.43337931e-01, 2.33178693e-01, 1.00000000e-09, ...,\n",
              "        1.00000000e-09, 1.00000000e-09, 1.00000000e-09],\n",
              "       [2.58368488e-01, 7.10659908e-02, 1.00000000e-09, ...,\n",
              "        1.00000000e-09, 1.00000000e-09, 1.00000000e-09],\n",
              "       ...,\n",
              "       [2.67528495e-01, 3.48822200e-01, 1.00000000e-09, ...,\n",
              "        1.00000000e-09, 1.00000000e-09, 1.00000000e-09],\n",
              "       [1.28621302e-01, 1.21752454e-01, 5.77347050e-02, ...,\n",
              "        1.00000000e-09, 1.00000000e-09, 1.00000000e-09],\n",
              "       [5.48420938e+00, 1.28950377e+00, 1.00000000e-09, ...,\n",
              "        4.08929846e-02, 1.00000000e-09, 1.00000000e-09]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentiment dataset before adding constant"
      ],
      "metadata": {
        "id": "Og-OK5OscWgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GaussianNaiveBayes()\n",
        "model.fit(X_train_counts2.toarray(), sentiment)\n",
        "p=model.predict(X_new_counts2[:].toarray())\n",
        "\n",
        "accur=np.mean(p == t_sentiment[:]%3)\n",
        "accur\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MOr8nMpcSm9",
        "outputId": "a26ebb1a-7908-4798-c6c0-29df00ab5d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5710306406685237"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.sigma[:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvnaU3YQcgk9",
        "outputId": "9f1dc00b-c7c3-42c7-9b6b-3ac46f43acfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.03757633, 0.02460091, 0.02460091, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.01990468, 0.02814389, 0.        , ..., 0.01407613, 0.01407613,\n",
              "        0.03147269]])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbkeKUHnj5Hi",
        "outputId": "9ab4f6f5-59d4-4f6f-a61d-1a00eb508c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentiment dataset gaussian with count occurences accuracy "
      ],
      "metadata": {
        "id": "NzqcgPAsVkHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GaussianNaiveBayes()\n",
        "model.fit(X_train_counts2.toarray(), sentiment)\n",
        "p=model.predict(X_new_counts2[:].toarray())\n",
        "\n",
        "accur=np.mean(p == t_sentiment[:]%3)\n",
        "accur\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YvEt6zeEBAO",
        "outputId": "f57dc49d-5175-4fc2-85a2-09d8ccf7a13b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5710306406685237"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.sigma[:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxX3HmuKEvUi",
        "outputId": "bb17e098-734e-4ebf-97fe-b1759ad41e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.75763358e-02, 2.46009062e-02, 2.46009062e-02, ...,\n",
              "        1.00000000e-09, 1.00000000e-09, 1.00000000e-09],\n",
              "       [1.99046852e-02, 2.81438939e-02, 1.00000000e-09, ...,\n",
              "        1.40761333e-02, 1.40761333e-02, 3.14726940e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentiment dataset gaussian with tfidf accuracy"
      ],
      "metadata": {
        "id": "NV9cNJTRWvbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GaussianNaiveBayes()\n",
        "model.fit(X_train_tfidf2.toarray(), sentiment)\n",
        "p=model.predict(X_new_tfidf2[:].toarray())\n",
        "\n",
        "accur=np.mean(p == t_sentiment[:]%3)\n",
        "accur\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpyHlrL-HpyB",
        "outputId": "30f6b391-e4af-4448-8ce5-e55de87fc7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5598885793871866"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentiment dataset gaussian count with stop words"
      ],
      "metadata": {
        "id": "aGy4MQPKZKKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GaussianNaiveBayes()\n",
        "model.fit(X_train_counts2.toarray(), sentiment)\n",
        "p=model.predict(X_new_counts2[:].toarray())\n",
        "\n",
        "accur=np.mean(p == t_sentiment[:]%3)\n",
        "accur\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEuf7-4oZJLf",
        "outputId": "6f0dc596-d6b7-42aa-c845-4d1a1953c896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5738161559888579"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentiment gaussian tfidf stop words"
      ],
      "metadata": {
        "id": "7kP7OIAgaLTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GaussianNaiveBayes()\n",
        "model.fit(X_train_tfidf2.toarray(), sentiment)\n",
        "p=model.predict(X_new_tfidf2[:].toarray())\n",
        "\n",
        "accur=np.mean(p == t_sentiment[:]%3)\n",
        "accur"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NQ6S012aOg4",
        "outputId": "fa517673-29a6-4405-adc5-6844ae194ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5654596100278552"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GaussianNaiveBayes()\n",
        "model.fit(X_train_counts[:].toarray(), training20.target[:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEK0x_yJ_p1z",
        "outputId": "e839b8b4-2456-44ca-b0bf-ca7eff718925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.GaussianNaiveBayes at 0x7f82248ac550>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.sigma[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mseuv-KS_w3f",
        "outputId": "1dcbef88-6ca7-417a-d6a6-dcf607d17d99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.00000000e-09, 4.19319680e-01, 1.00000000e-09, ...,\n",
              "        1.00000000e-09, 1.00000000e-09, 1.00000000e-09],\n",
              "       [3.43337931e-01, 2.33178693e-01, 1.00000000e-09, ...,\n",
              "        1.00000000e-09, 1.00000000e-09, 1.00000000e-09],\n",
              "       [2.58368488e-01, 7.10659908e-02, 1.00000000e-09, ...,\n",
              "        1.00000000e-09, 1.00000000e-09, 1.00000000e-09],\n",
              "       ...,\n",
              "       [2.67528495e-01, 3.48822200e-01, 1.00000000e-09, ...,\n",
              "        1.00000000e-09, 1.00000000e-09, 1.00000000e-09],\n",
              "       [1.28621302e-01, 1.21752454e-01, 5.77347050e-02, ...,\n",
              "        1.00000000e-09, 1.00000000e-09, 1.00000000e-09],\n",
              "       [5.48420938e+00, 1.28950377e+00, 1.00000000e-09, ...,\n",
              "        4.08929846e-02, 1.00000000e-09, 1.00000000e-09]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p=model.predict(X_new_counts[:10].toarray())\n",
        "accur=np.mean(p == twenty_test.target[:10])\n",
        "accur"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar69Kwq5_8hs",
        "outputId": "93c4cc6b-47d1-42ed-e905-533d577d37b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: divide by zero encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in subtract\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = []\n",
        "n=10\n",
        "for i in range(1,753):  \n",
        "  y_prob = model.predict(X_new_counts[(i-1)*n:i*n].toarray())\n",
        "  accuracy = np.mean(y_prob == twenty_test.target[(i-1)*n:i*n])\n",
        "  l.append(accuracy)\n",
        "  print(f'test accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duJMcnTQAgf4",
        "outputId": "a07045c5-b357-4461-b3d1-fe2ff47c4a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.1\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.1\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 1.0\n",
            "test accuracy: 0.1\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.1\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.1\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 1.0\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 1.0\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.8\n",
            "test accuracy: 1.0\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.9\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.7\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.2\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.8\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.3\n",
            "test accuracy: 0.4\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.6\n",
            "test accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result of (1,1) n-gram with MultinomialNB on first dataset with best hyperparameters: 39.79%"
      ],
      "metadata": {
        "id": "g_bITdggA0yt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result of (1,1) n-gram with GaussianNB on first dataset with count + stopwords: 41.89%"
      ],
      "metadata": {
        "id": "OJfAbHQ0CYwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result of (1,1) n-gram with GaussianNB on first dataset with best hyperparameters: 42.55%"
      ],
      "metadata": {
        "id": "vhllvN3h7CrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK0TZDEoAuGD",
        "outputId": "9f9daf56-c7d5-46e0-c715-af72ce9986bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.550531914893617"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Naive Bayes accuracy from first dataset"
      ],
      "metadata": {
        "id": "z_Z4F2UqXrN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(0.549466*7500+0.625*32)/7532"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aMEjwT5Xq13",
        "outputId": "5782a743-b881-499d-9a2e-f9dcbefc13b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5497869091874668"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "clf = BernoulliNB().fit(X_train_tfidf, training20.target)\n",
        "docs_test = X_new_tfidf\n",
        "predicted = clf.predict(docs_test)\n",
        "np.mean(predicted == twenty_test.target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfwY2_fisU7f",
        "outputId": "fbb6afef-435b-428c-a54d-4b6e1d940103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4579129049389272"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "clf = GaussianNB().fit(X_train_tfidf[:].toarray(), training20.target[:])\n",
        "docs_test = X_new_tfidf\n",
        "predicted = clf.predict(docs_test[0:].toarray())\n",
        "np.mean(predicted == twenty_test.target[0:])"
      ],
      "metadata": {
        "id": "YpL62bVnmiEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB().fit(X_train_tfidf, training20.target)\n",
        "docs_test = X_new_tfidf\n",
        "predicted = clf.predict(docs_test)\n",
        "np.mean(predicted == twenty_test.target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m0ggVpdzWZ8",
        "outputId": "43609c69-ff22-4be3-b7f3-7aa985b98ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6779075942644716"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection\n",
        "model = GaussianNaiveBayes()\n",
        "model.fit(X_train_tfidf2.toarray(), sentiment)\n",
        "y_prob = model.predict(X_new_tfidf2.toarray())\n",
        "y_pred = np.argmax(y_prob, 1)\n",
        "accuracy = np.sum(y_pred == t_sentiment)/y_pred.shape[0]\n",
        "print(f'test accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kUh5ZST7YOa",
        "outputId": "9b23807c-bab4-4aa1-d24a-a87eacc6799e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in true_divide\n",
            "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:263: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  keepdims=keepdims, where=where)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in true_divide\n",
            "  subok=False)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:252: RuntimeWarning: invalid value encountered in true_divide\n",
            "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.49303621169916434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "clf = GaussianNB().fit(X_train_tfidf2.toarray(), sentiment)\n",
        "docs_test = X_new_tfidf2\n",
        "predicted = clf.predict(docs_test.toarray())\n",
        "np.mean(predicted == t_sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycQ0i8se9Atf",
        "outputId": "de77627a-5ec2-4cd5-b0df-65f76372cc9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5738161559888579"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB().fit(X_train_tfidf2, sentiment)\n",
        "docs_test = X_new_tfidf2\n",
        "predicted = clf.predict(docs_test)\n",
        "np.mean(predicted == t_sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByDOOfU29ZkN",
        "outputId": "bfbaed53-b150-4963-830e-4225f8257bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7520891364902507"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "clf = BernoulliNB().fit(X_train_tfidf2, sentiment)\n",
        "docs_test = X_new_tfidf2\n",
        "predicted = clf.predict(docs_test)\n",
        "np.mean(predicted == t_sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQjECEgL3Hqz",
        "outputId": "4096e9c0-d848-4dec-cdf5-168b983b5001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7493036211699164"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multionomial Naive Bayes Implementation"
      ],
      "metadata": {
        "id": "hYzlq7JmXVbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "# np.random.seed(1234)\n",
        "\n",
        "\n",
        "class MultinomialNaiveBayes:\n",
        "    \n",
        "    def __init__(self):\n",
        "        return\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "      #Compute all the priors for each class: number of documents of one class divided by total number of documents\n",
        "      #Calculate Conditional probabilities: number of each word in each class, do +1 on numerator and +num of unique words in denominator for smoothing\n",
        "      #To predict a word, calculate probability of each class by doing the below:\n",
        "      #Multiply prior of the class by conditional probability of each word being in said class in that document\n",
        "      #Choose the max probability as the predicted class\n",
        "      #Have to somehow do this in logarithms i have no clue lol\n",
        "      train = x\n",
        "      doc, features = train.shape\n",
        "      # Prior\n",
        "      unique, counts = np.unique(y, return_counts=True)\n",
        "      prior = dict(zip(unique, counts))\n",
        "      vocab = {}\n",
        "      hm = {}\n",
        "      for i in prior:\n",
        "        prior[i] /= doc\n",
        "        hm[i] = {}\n",
        "        vocab[i] = 0\n",
        "        for j in range(features):\n",
        "          # hm[i][j] = 1\n",
        "          hm[i][j] = 0\n",
        "      # Conditional probabilities\n",
        "      # Create a dictionary with every class in it\n",
        "      for j in range(len(train)):\n",
        "        vocab[y[j]] += np.count_nonzero(train[j])\n",
        "        # if j % 500 == 0:\n",
        "        #   print(j)\n",
        "        nz = np.nonzero(train[j])[0]\n",
        "        for k in nz:\n",
        "          hm[y[j]][k] += (train[j][k])\n",
        "      for i in hm:\n",
        "        for j in hm[i]:\n",
        "          # hm[i][j] /= (doc+vocab[i])\n",
        "          hm[i][j] /= vocab[i]\n",
        "      self.hm = hm\n",
        "      self.prior = prior\n",
        "\n",
        "      return self\n",
        "\n",
        "def logsumexp(Z):                                                # dimension C x N\n",
        "    Zmax = np.max(Z,axis=0)[None,:]                              # max over C\n",
        "    log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=0))\n",
        "    return log_sum_exp\n",
        "\n",
        "def predict(self, xt):\n",
        "    from math import log, exp, pow\n",
        "    result = []\n",
        "    first = list(self.hm.keys())[0]\n",
        "    # for each line to predict\n",
        "    for i in range(len(xt)):\n",
        "      # if i % 5 == 0:\n",
        "      #   print(i)\n",
        "      # for each class it could be\n",
        "      tmp = first\n",
        "      best = float('-inf')\n",
        "      nz = np.nonzero(xt[i])[0]\n",
        "      for j in self.hm:\n",
        "        # calculate probability of that line being that class\n",
        "        # and then choose the class with highest probability\n",
        "        prob = log(self.prior[j])\n",
        "        for k in range(len(nz)):\n",
        "          # print(xt[i][nz[k]] * log(self.hm[j][nz[k]]))\n",
        "          if (self.hm[j][nz[k]] == 0.0):\n",
        "              prob += log(1)\n",
        "          else:\n",
        "              prob += xt[i][nz[k]] * log(self.hm[j][nz[k]])\n",
        "          # prob += log(math.pow(self.hm[j][nz[k]],xt[i][nz[k]]))\n",
        "        if best < prob:\n",
        "          best = prob\n",
        "          tmp = j\n",
        "      result.append(tmp)\n",
        "    res = np.array(result)\n",
        "    return res                                      # dimension N x C\n",
        "\n",
        "MultinomialNaiveBayes.predict = predict"
      ],
      "metadata": {
        "id": "vxdYgfp-XUkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second dataset MultinomialNB"
      ],
      "metadata": {
        "id": "C04d3jnNvldf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection\n",
        "model = MultinomialNaiveBayes()\n",
        "limit_training_set2 = floor(X_train_tfidf2.shape[0]*0.8)\n",
        "model.fit(X_train_tfidf2[:limit_training_set2].toarray(), sentiment[:limit_training_set2])\n",
        "y_prob = model.predict(X_new_tfidf2.toarray())\n",
        "accuracy = np.mean(y_prob == t_sentiment)\n",
        "print(f'test accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "W5bpWXJuKSUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First Dataset MultinomialNB"
      ],
      "metadata": {
        "id": "7ZT-dtCgvOEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultinomialNaiveBayes()\n",
        "from math import floor\n",
        "limit_training_set = floor(X_train_counts.shape[0]*1.0)\n",
        "model.fit(X_train_counts[:limit_training_set].toarray(), training20.target[:limit_training_set])\n",
        "y_prob = model.predict(X_new_counts.toarray())\n",
        "accuracy = np.mean(y_prob == twenty_test.target)\n",
        "print(f'test accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "w329AzxcaIMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GaussianNaiveBayes()\n",
        "from math import floor\n",
        "#limit_training_set = floor(X_train_coun.shape[0]*1.0)\n",
        "model.fit(X_train_tfidf[:].toarray(), training20.target[:])\n",
        "y_prob = model.predict(X_new_tfidf.toarray())\n",
        "accuracy = np.mean(y_prob == twenty_test.target)\n",
        "print(f'test accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "rIHJrX7tbWgK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}